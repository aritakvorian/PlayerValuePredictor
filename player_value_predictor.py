# -*- coding: utf-8 -*-
"""Player Value Predictor

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sbH13IDzx7ha6UHtNBqxmZ97PuYZbK-F

# APPENDIX A

# Player Value Predictor

## Preparation

### Imports and Data Load

Here, I'll be importing libraries and loading the file.
"""

import pandas as pd
import math
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
pd.set_option("display.max_columns", None, "display.max_rows", None)

from google.colab import drive
drive.mount('/content/gdrive')

cd gdrive/MyDrive/UT/Plan\ II\ \(General\)/Thesis/Model

raw = pd.read_csv('final_dataset.csv')

display(raw.head())
print(f"Total number of rows: {raw.shape[0]}\nTotal number of columns: {raw.shape[1]}")

"""### Cleaning

First going to take a look at the data.
"""

display(raw['Position'].value_counts())

# Print all columns

for col in raw.columns:
  print(col)

"""Since there are not many (basically zero) stats for GKs, going to remove them from the dataset."""

outfield = raw[raw['Position']!='Goalkeeper']
display(outfield['Position'].value_counts())
print(f"Total number of rows: {outfield.shape[0]}\nTotal number of columns: {outfield.shape[1]}")

"""Going to remove traits like Nationality and Club, and keep league, since league is more of an indicator of performance rather than a player's nationality or team (league will account for club)."""

outfield = outfield.drop(columns=['Club', 'Squad (20/21)', 'Nation'])
display(outfield.head())

"""Going to rename the positions."""

outfield['Position'] = outfield['Position'].replace('attack', 'F')
outfield['Position'] = outfield['Position'].replace('midfield', 'M')
outfield['Position'] = outfield['Position'].replace('Defender', 'D')
display(outfield.head())
print(outfield['Position'].value_counts())

"""Check on leagues to one-hot-encode."""

display(outfield['League'].value_counts())

dummy = pd.get_dummies(outfield.League, prefix='League')
print(dummy.head())

outfield = pd.concat([outfield.drop('League', axis=1), pd.get_dummies(outfield['League'], prefix='League')], axis=1)
outfield.head()

"""Now going to check for NaN values in the data"""

outfield.isna().sum()

"""If a player has NaN for matches played, we will remove their entire row from the dataset"""

outfield = outfield[outfield['MP (20/21)'].notna()]

outfield.isna().sum()

"""For remaining NaN, going to replace with 0s"""

outfield = outfield.fillna(value=0)
outfield.isna().sum()

"""Going to now one-hot encode the position as well, to make all features numeric."""

outfield = pd.concat([outfield.drop('Position', axis=1), pd.get_dummies(outfield['Position'], prefix='Position')], axis=1)
outfield.head()

"""All features (besides name) are now numeric, and data is ready for EDA.

## EDA and Feature Selection

### EDA

Let's map the values to see if we need to log(transform) them later on.
"""

plt.hist(outfield['Value'])

plt.hist(np.log(outfield['Value']))

"""This data is much cleaner, and will probably have to use a log-transformation for final model.

Going to make a ton of scatter plots to see how variables correlate with value
"""

len(outfield.columns)

"""144 columns (excluding value and player name)"""

col = outfield.columns.drop(['Value', 'Player'])
col

#pp = sns.pairplot(data=outfield, y_vars=col,x_vars='Value')

"""Not immediately clear if there are any featuers that show a strong correlation. Let's try with log(transformation) of values."""

#outfield['Log Value'] = np.log(outfield['Value'])

#pp = sns.pairplot(data=outfield, y_vars=col, x_vars=['Log Value'])

#outfield = outfield.drop('Log Value', 1)

"""The variables show much more correlation with log(transform).

### Feature Selection

Now going to create a heatmap to see if I should eliminate some features from the model.
"""

outfield['Contract Years Left'] = pd.to_numeric(outfield['Contract Years Left'], errors='coerce')
outfield['Log Value'] = np.log(outfield['Value'])

temp = outfield.loc[:, outfield.columns != 'Player']
temp = temp.loc[:, temp.columns != 'Value']

fig, ax = plt.subplots(figsize=(12, 40))
sns.heatmap(temp.corr()[['Log Value']].sort_values('Log Value'),
 vmax=1, vmin=-1, cmap='YlGnBu', annot=True, ax=ax);
ax.invert_yaxis()

"""Going to run base models (no tuning) for the following sets of features:



*   All features
*   Best 110
*   Best 80
*   Best 50
*   Best 20



The 'Best' features are going to be calculated using the SelectKBest function of Sci-kit learn. Will be using Log_Value as target.
"""

# Create copy of DF with all features, but without player name, value, and contract years left

all_features = outfield.copy()
all_features = all_features.drop(columns=['Player', 'Value', 'Contract Years Left'])

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

# Get Best 110 features

select_oneten = SelectKBest(score_func=f_regression, k = 110)
oneten = select_oneten.fit_transform(all_features.drop(columns=['Log Value']), all_features['Log Value'])

filter = select_oneten.get_support()
feats = np.array(all_features.drop(columns=['Log Value']).columns)

print(f'Selected best 110 features: {feats[filter]}')

oneten_features = all_features.copy()
oneten_features.drop(oneten_features.columns.difference(feats[filter]), 1, inplace=True)
oneten_features['Log Value'] = all_features['Log Value'].copy()

# Get Best 80 features

select_eighty = SelectKBest(score_func=f_regression, k = 80)
eighty = select_eighty.fit_transform(all_features.drop(columns=['Log Value']), all_features['Log Value'])

filter = select_eighty.get_support()
feats = np.array(all_features.drop(columns=['Log Value']).columns)

print(f'Selected best 80 features: {feats[filter]}')

eighty_features = all_features.copy()
eighty_features.drop(eighty_features.columns.difference(feats[filter]), 1, inplace=True)
eighty_features['Log Value'] = all_features['Log Value'].copy()

# Get Best 50 features

select_fifty = SelectKBest(score_func=f_regression, k = 50)
fifty = select_fifty.fit_transform(all_features.drop(columns=['Log Value']), all_features['Log Value'])

filter = select_fifty.get_support()
feats = np.array(all_features.drop(columns=['Log Value']).columns)

print(f'Selected best 50 features: {feats[filter]}')

fifty_features = all_features.copy()
fifty_features.drop(fifty_features.columns.difference(feats[filter]), 1, inplace=True)
fifty_features['Log Value'] = all_features['Log Value'].copy()

# Get Best 20 features

select_twenty = SelectKBest(score_func=f_regression, k = 20)
fifty = select_twenty.fit_transform(all_features.drop(columns=['Log Value']), all_features['Log Value'])

filter = select_twenty.get_support()
feats = np.array(all_features.drop(columns=['Log Value']).columns)

print(f'Selected best 20 features: {feats[filter]}')

twenty_features = all_features.copy()
twenty_features.drop(twenty_features.columns.difference(feats[filter]), 1, inplace=True)
twenty_features['Log Value'] = all_features['Log Value'].copy()

"""Now have five dataframes, each containing a different # of features."""

print('All Features')
display(all_features.head())

print('110 Features')
display(oneten_features.head())

print('80 Features')
display(eighty_features.head())

print('50 Features')
display(fifty_features.head())

print('20 Features')
display(twenty_features.head())

all_features.to_csv('all_features.csv', encoding='utf-8', index=False)
oneten_features.to_csv('oneten_features.csv', encoding='utf-8', index=False)
eighty_features.to_csv('eighty_features.csv', encoding='utf-8', index=False)
fifty_features.to_csv('fifty_features.csv', encoding='utf-8', index=False)
twenty_features.to_csv('twenty_features.csv', encoding='utf-8', index=False)

"""## Model Testing

### Setup

Will put each of the five data frames through the following models (with minimal tuning - real tuning will come after choosing the best type of model):

*   Multiple (Simple) Linear Regression
*   Lasso Regression
*   Ridge Regression
*   XGBoost Regressor
*   ElasticNet Regression
*   CatBoost Regressor
*   Random Forest Regressor
*   K-Nearest Neighbors

The regression metric used will be the Mean Absolute Percentage Error (MAPE), but, because I have log-transformed the data, it will reflect the log percentage.

Each dataset will be split into train and test sets (test_size being 10% of data). Will train on the 90%, and evaluate the MAPE on the remaining 10%).
"""

models_list = {'Feature #': ['All_Features', '110_Features', '80_Features', '50_Features', '20_Features']}

results = pd.DataFrame(data = models_list)

linreg = np.zeros(5)
results['LinReg'] = linreg.copy()
results['Lasso'] = linreg.copy()
results['Ridge'] = linreg.copy()
results['XGBoost'] = linreg.copy()
results['ElasticNet'] = linreg.copy()
results['CatBoost'] = linreg.copy()
results['RandomForest'] = linreg.copy()
results['KNN'] = linreg.copy()

results.head()

# Creating Train-Test Splits

X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(all_features.drop(columns=['Log Value']), all_features['Log Value'], random_state = 42, test_size = 0.10)
X_train_oneten, X_test_oneten, y_train_oneten, y_test_oneten = train_test_split(oneten_features.drop(columns=['Log Value']), oneten_features['Log Value'], random_state = 42, test_size = 0.10)
X_train_eighty, X_test_eighty, y_train_eighty, y_test_eighty = train_test_split(eighty_features.drop(columns=['Log Value']), eighty_features['Log Value'], random_state = 42, test_size = 0.10)
X_train_fifty, X_test_fifty, y_train_fifty, y_test_fifty = train_test_split(fifty_features.drop(columns=['Log Value']), fifty_features['Log Value'], random_state = 42, test_size = 0.10)
X_train_twenty, X_test_twenty, y_train_twenty, y_test_twenty = train_test_split(twenty_features.drop(columns=['Log Value']), twenty_features['Log Value'], random_state = 42, test_size = 0.10)

"""### Multiple (Simple) Linear Regression"""

from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression

model = LinearRegression()

## All Features - MLR

model.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, model.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[0,1] = mape_test*100

## 110 Features - MLR

model.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, model.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test}')
results.iat[1,1] = mape_test*100

## 80 Features - MLR

model.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, model.predict(X_test_eighty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[2,1] = mape_test*100

## 50 Features - MLR

model.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, model.predict(X_test_fifty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[3,1] = mape_test*100

## 20 Features - MLR

model.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, model.predict(X_test_twenty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[4,1] = mape_test*100

results

"""### Lasso Regression"""

from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Lasso

grid = {'alpha': [0, 0.01, 0.1, 1]}
cv = RepeatedKFold(n_splits = 3, n_repeats = 3, random_state = 42)
model = Lasso()

search = GridSearchCV(model, grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs =-1)

## All Features - Lasso

res_all = search.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, search.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test} with Hyperparameters: {res_all.best_params_}')
results.iat[0,2] = mape_test*100

## 110 Features - Lasso

res_oneten = search.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, search.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test} with Hyperparameters: {res_oneten.best_params_}')
results.iat[1,2] = mape_test*100

## 80 Features - Lasso

res_eighty = search.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, search.predict(X_test_eighty))

print(f'Test Data MAPE for 80 Features: {mape_test} with Hyperparameters: {res_eighty.best_params_}')
results.iat[2,2] = mape_test*100

## 50 Features - Lasso

res_fifty = search.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, search.predict(X_test_fifty))

print(f'Test Data MAPE for 50 Features: {mape_test} with Hyperparameters: {res_fifty.best_params_}')
results.iat[3,2] = mape_test*100

## 20 Features - Lasso

res_twenty = search.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, search.predict(X_test_twenty))

print(f'Test Data MAPE for 20 Features: {mape_test} with Hyperparameters: {res_twenty.best_params_}')
results.iat[4,2] = mape_test*100

"""### Ride Regression"""

from sklearn.linear_model import Ridge

grid = {'alpha': np.arange(0, 1, 0.05)}
cv = RepeatedKFold(n_splits = 3, n_repeats = 3, random_state = 42)
model = Ridge()

search = GridSearchCV(model, grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs =-1)

## All Features - Ridge

res_all = search.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, search.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test} with Hyperparameters: {res_all.best_params_}')
results.iat[0,3] = mape_test*100

## 110 Features - Ridge

res_oneten = search.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, search.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test} with Hyperparameters: {res_oneten.best_params_}')
results.iat[1,3] = mape_test*100

## 80 Features - Ridge

res_eighty = search.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, search.predict(X_test_eighty))

print(f'Test Data MAPE for 80 Features: {mape_test} with Hyperparameters: {res_eighty.best_params_}')
results.iat[2,3] = mape_test*100

## 50 Features - Ridge

res_fifty = search.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, search.predict(X_test_fifty))

print(f'Test Data MAPE for 50 Features: {mape_test} with Hyperparameters: {res_fifty.best_params_}')
results.iat[3,3] = mape_test*100

## 20 Features - Ridge

res_twenty = search.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, search.predict(X_test_twenty))

print(f'Test Data MAPE for 20 Features: {mape_test} with Hyperparameters: {res_twenty.best_params_}')
results.iat[4,3] = mape_test*100

"""### XGBoost"""

import xgboost
from xgboost import XGBRegressor

model = XGBRegressor(objective='reg:squarederror')

## All Features - XGBoost

model.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, model.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[0,4] = mape_test*100

## 110 Features - XGBoost

model.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, model.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test}')
results.iat[1,4] = mape_test*100

## 80 Features - XGBoost

model.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, model.predict(X_test_eighty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[2,4] = mape_test*100

## 50 Features - XGBoost

model.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, model.predict(X_test_fifty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[3,4] = mape_test*100

## 20 Features - XGBoost

model.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, model.predict(X_test_twenty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[4,4] = mape_test*100

"""### ElasticNet"""

from sklearn.linear_model import ElasticNet

grid = {'alpha': np.arange(0, 1, 0.2), 
        'l1_ratio': np.arange(0, 1, 0.5)}
cv = RepeatedKFold(n_splits = 3, n_repeats = 3, random_state = 42)
model = ElasticNet()

search = GridSearchCV(model, grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs =-1)

## All Features - ElasticNet

res_all = search.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, search.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test} with Hyperparameters: {res_all.best_params_}')
results.iat[0,5] = mape_test*100

## 110 Features - ElasticNet

res_oneten = search.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, search.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test} with Hyperparameters: {res_oneten.best_params_}')
results.iat[1,5] = mape_test*100

## 80 Features - ElasticNet

res_eighty = search.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, search.predict(X_test_eighty))

print(f'Test Data MAPE for 80 Features: {mape_test} with Hyperparameters: {res_eighty.best_params_}')
results.iat[2,5] = mape_test*100

## 50 Features - ElasticNet

res_fifty = search.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, search.predict(X_test_fifty))

print(f'Test Data MAPE for 50 Features: {mape_test} with Hyperparameters: {res_fifty.best_params_}')
results.iat[3,5] = mape_test*100

## 20 Features - ElasticNet

res_twenty = search.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, search.predict(X_test_twenty))

print(f'Test Data MAPE for 20 Features: {mape_test} with Hyperparameters: {res_twenty.best_params_}')
results.iat[4,5] = mape_test*100

results

"""### CatBoost """

pip install catboost

import catboost
from catboost import CatBoostRegressor

model = CatBoostRegressor(loss_function = 'MAPE')

## All Features - CatBoost

model.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, model.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[0,6] = mape_test*100

## 110 Features - CatBoost

model.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, model.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test}')
results.iat[1,6] = mape_test*100

## 80 Features - CatBoost

model.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, model.predict(X_test_eighty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[2,6] = mape_test*100

## 50 Features - CatBoost

model.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, model.predict(X_test_fifty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[3,6] = mape_test*100

## 20 Features - CatBoost

model.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, model.predict(X_test_twenty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[4,6] = mape_test*100

results

"""### Random Forest"""

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(random_state = 42, criterion='absolute_error')

## All Features - Random Forest

model.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, model.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[0,7] = mape_test*100

## 110 Features - Random Forest

model.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, model.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test}')
results.iat[1,7] = mape_test*100

## 80 Features - Random Forest

model.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, model.predict(X_test_eighty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[2,7] = mape_test*100

## 50 Features - Random Forest

model.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, model.predict(X_test_fifty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[3,7] = mape_test*100

## 20 Features - Random Forest

model.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, model.predict(X_test_twenty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[4,7] = mape_test*100

results

"""### K-Nearest Neighbors



"""

from sklearn.neighbors import KNeighborsRegressor

model = KNeighborsRegressor()

## All Features - KNN

model.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, model.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[0,8] = mape_test*100

## 110 Features - KNN

model.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, model.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test}')
results.iat[1,8] = mape_test*100

## 80 Features - KNN

model.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, model.predict(X_test_eighty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[2,8] = mape_test*100

## 50 Features - KNN

model.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, model.predict(X_test_fifty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[3,8] = mape_test*100

## 20 Features - KNN

model.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, model.predict(X_test_twenty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[4,8] = mape_test*100

results

"""### Conclusion of Model Selection"""

display(results)

plt.figure(figsize=(15,10))
plt.plot(results['Features #'], results['LinReg'], label='Linear Regression')
plt.plot(results['Features #'], results['Lasso'], label='Lasso Regression')
plt.plot(results['Features #'], results['Ridge'], label='Ridge Regression')
plt.plot(results['Features #'], results['XGBoost'], label='XGBoost')
plt.plot(results['Features #'], results['ElasticNet'], label='ElasticNet Regression')
plt.plot(results['Features #'], results['CatBoost'], label='CatBoost')
plt.plot(results['Features #'], results['RandomForest'], label='Random Forest')
plt.plot(results['Features #'], results['KNN'], label='K-Nearest Neighbors')
plt.xlabel('Number of Features')
plt.ylabel('Mean Absolute Percentage Error (MAPE) for Log Value')
plt.title('Model Framework MAPE Performance by # of Features')
plt.legend()

"""CatBoost seems to work the best, and on the 110 feature set. This indicates that there is some best value for feature #s. Going to try some more #s of features between All and 110 for CatBoost to determine the best data set.

## Further Feature Selection

### Features

Going to compare results for CatBoost for features counts:

 All, 130, 125, 120, 115, and 110.

*   All
*   130
*   125
*   120
*   115
*   110

All and 110 are already calculated
"""

# 130 Feats

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

select_onethirty = SelectKBest(score_func=f_regression, k = 130)
onethirty = select_onethirty.fit_transform(all_features.drop(columns=['Log Value']), all_features['Log Value'])

filter = select_onethirty.get_support()
feats = np.array(all_features.drop(columns=['Log Value']).columns)

#print(f'Selected best 130 features: {feats[filter]}')

onethirty_features = all_features.copy()
onethirty_features.drop(onethirty_features.columns.difference(feats[filter]), 1, inplace=True)
onethirty_features['Log Value'] = all_features['Log Value'].copy()

# 125 Feats

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

select_onetwofive = SelectKBest(score_func=f_regression, k = 125)
onetwofive = select_onetwofive.fit_transform(all_features.drop(columns=['Log Value']), all_features['Log Value'])

filter = select_onetwofive.get_support()
feats = np.array(all_features.drop(columns=['Log Value']).columns)

#print(f'Selected best 125 features: {feats[filter]}')

onetwofive_features = all_features.copy()
onetwofive_features.drop(onetwofive_features.columns.difference(feats[filter]), 1, inplace=True)
onetwofive_features['Log Value'] = all_features['Log Value'].copy()

# 120 Feats

select_onetwenty = SelectKBest(score_func=f_regression, k = 120)
onetwenty = select_onetwenty.fit_transform(all_features.drop(columns=['Log Value']), all_features['Log Value'])

filter = select_onetwenty.get_support()
feats = np.array(all_features.drop(columns=['Log Value']).columns)

#print(f'Selected best 120 features: {feats[filter]}')

onetwenty_features = all_features.copy()
onetwenty_features.drop(onetwenty_features.columns.difference(feats[filter]), 1, inplace=True)
onetwenty_features['Log Value'] = all_features['Log Value'].copy()

# 115 Feats

select_onefifteen = SelectKBest(score_func=f_regression, k = 115)
onefifteen = select_onefifteen.fit_transform(all_features.drop(columns=['Log Value']), all_features['Log Value'])

filter = select_onefifteen.get_support()
feats = np.array(all_features.drop(columns=['Log Value']).columns)

#print(f'Selected best 115 features: {feats[filter]}')

onefifteen_features = all_features.copy()
onefifteen_features.drop(onefifteen_features.columns.difference(feats[filter]), 1, inplace=True)
onefifteen_features['Log Value'] = all_features['Log Value'].copy()

"""### Model Testing pt 2

Going to repeat the process as before, but only for CatBoost
"""

X_train_onethirty, X_test_onethirty, y_train_onethirty, y_test_onethirty = train_test_split(onethirty_features.drop(columns=['Log Value']), onethirty_features['Log Value'], random_state = 42, test_size = 0.10)
X_train_onetwofive, X_test_onetwofive, y_train_onetwofive, y_test_onetwofive = train_test_split(onetwofive_features.drop(columns=['Log Value']), onetwofive_features['Log Value'], random_state = 42, test_size = 0.10)
X_train_onetwenty, X_test_onetwenty, y_train_onetwenty, y_test_onetwenty = train_test_split(onetwenty_features.drop(columns=['Log Value']), onetwenty_features['Log Value'], random_state = 42, test_size = 0.10)
X_train_onefifteen, X_test_onefifteen, y_train_onefifteen, y_test_onefifteen = train_test_split(onefifteen_features.drop(columns=['Log Value']), onefifteen_features['Log Value'], random_state = 42, test_size = 0.10)

models_list = {'Feature #': ['All_Features', '130_Features', '125_Features','120_Features', '115_Features', '110_Features']}

results_two = pd.DataFrame(data = models_list)

linreg = np.zeros(6)
results_two['CatBoost'] = linreg.copy()

results_two.iat[0, 1] = results.iloc[0, 6]
results_two.iat[5, 1] = results.iloc[1, 6]

display(results_two)

import catboost
from catboost import CatBoostRegressor

model = CatBoostRegressor(loss_function = 'MAPE')

## 130 Features - CatBoost

model.fit(X_train_onethirty, y_train_onethirty)

mape_test = mean_absolute_percentage_error(y_test_onethirty, model.predict(X_test_onethirty))

print(f'Test Data MAPE for 130 Features: {mape_test}')
results_two.iat[1,1] = mape_test*100

## 125 Features - CatBoost

model.fit(X_train_onetwofive, y_train_onetwofive)

mape_test = mean_absolute_percentage_error(y_test_onetwofive, model.predict(X_test_onetwofive))

print(f'Test Data MAPE for 125 Features: {mape_test}')
results_two.iat[2,1] = mape_test*100

## 120 Features - CatBoost

model.fit(X_train_onetwenty, y_train_onetwenty)

mape_test = mean_absolute_percentage_error(y_test_onetwenty, model.predict(X_test_onetwenty))

print(f'Test Data MAPE for 120 Features: {mape_test}')
results_two.iat[3,1] = mape_test*100

## 115 Features - CatBoost

model.fit(X_train_onefifteen, y_train_onefifteen)

mape_test = mean_absolute_percentage_error(y_test_onefifteen, model.predict(X_test_onefifteen))

print(f'Test Data MAPE for 115 Features: {mape_test}')
results_two.iat[4,1] = mape_test*100

results_two

"""### Conclusion

The results show the best feature # with a non-tuned CatBoost is 130 (barely). As a result, I am going to continue with a CatBoostRegression with 130 features.
"""

onethirty_features.to_csv('onethirty_features.csv', encoding='utf-8', index=False)

"""## Training and Tuning

### Baseline CatBoost

Going to start by seeing what the baseline (not tuned) model does against the test set. Then going to tune some of the hyperparams around what is listed.
"""

X_train, X_test, y_train, y_test = train_test_split(onethirty_features.drop(columns=['Log Value']), onethirty_features['Log Value'], random_state=42, test_size=0.1)

untuned = CatBoostRegressor(loss_function='MAPE')

untuned.fit(X_train, y_train)

print(f'Train MAPE: {mean_absolute_percentage_error(y_train, untuned.predict(X_train))*100} %')
print(f'Test MAPE: {mean_absolute_percentage_error(y_test, untuned.predict(X_test))*100} %')

untuned.get_all_params()

"""Clearly there is some overfitting going on. Going to try tuning to see what can be done.

### Tuning 1
"""

cv = RepeatedKFold(n_splits = 3, random_state = 42, n_repeats=2)
model = CatBoostRegressor(logging_level='Silent', loss_function='MAPE')

grid = {'depth' : [5,6,7],
        'learning_rate' : [0.01, 0.025, 0.035],
        'iterations'    : [1000, 1250, 1500],
        'l2_leaf_reg' : [2,3]
       }


search = GridSearchCV(model, grid, scoring='neg_mean_absolute_percentage_error', cv=cv, verbose=3)

tuning_one = search.fit(X_train, y_train)

print(tuning_one.best_params_)

# Used to avoid running tuning

tuning_one = CatBoostRegressor(depth=5, iterations=1500, l2_leaf_reg = 2, learning_rate = 0.035)
tuning_one.fit(X_train, y_train)

print(f'Train MAPE: {mean_absolute_percentage_error(y_train, tuning_one.predict(X_train))*100} %')
print(f'Test MAPE: {mean_absolute_percentage_error(y_test, tuning_one.predict(X_test))*100} %')

"""### Tuning 2 (Overfit Prevention)"""

cv = RepeatedKFold(n_splits = 3, random_state = 42, n_repeats=2)
model = CatBoostRegressor(logging_level='Silent', loss_function='MAPE')

grid = {'depth' : [5,6],
        'learning_rate' : [0.025, 0.03, 0.035],
        'iterations'    : [1250, 1500],
        'l2_leaf_reg' : [2,3]
       }

eval_pool = Pool(X_test, y_test)

fit_params = {'eval_set' : eval_pool,
              'early_stopping_rounds' : 10
             }


search = GridSearchCV(model, grid, scoring='neg_mean_absolute_percentage_error', cv=cv, verbose=3)

tuning_two = search.fit(X_train, y_train, **fit_params)

print(tuning_two.best_params_)

# Used to avoid running tuning

tuning_two = CatBoostRegressor(depth=5, iterations=1250, l2_leaf_reg = 3, learning_rate = 0.03)
tuning_two.fit(X_train, y_train)

print(f'Train MAPE: {mean_absolute_percentage_error(y_train, tuning_two.predict(X_train))*100} %')
print(f'Test MAPE: {mean_absolute_percentage_error(y_test, tuning_two.predict(X_test))*100} %')

"""### Tuning 3 (More Specific of 1)"""

cv = RepeatedKFold(n_splits = 3, random_state = 42, n_repeats=2)
model = CatBoostRegressor(logging_level='Silent', loss_function='MAPE')

grid = {'depth' : [5],
        'learning_rate' : np.arange(0.025, 0.035, 0.001),
        'iterations'    : [1250, 1350, 1500],
       }

eval_pool = Pool(X_test, y_test)

fit_params = {'eval_set' : eval_pool,
              'early_stopping_rounds' : 10
             }


search = GridSearchCV(model, grid, scoring='neg_mean_absolute_percentage_error', cv=cv, verbose=3)

tuning_three = search.fit(X_train, y_train, **fit_params)

print(tuning_three.best_params_)

# Used to avoid running tuning

tuning_three = CatBoostRegressor(depth=5, iterations=1250,  learning_rate = 0.032)
tuning_three.fit(X_train, y_train)

print(f'Train MAPE: {mean_absolute_percentage_error(y_train, tuning_three.predict(X_train))*100} %')
print(f'Test MAPE: {mean_absolute_percentage_error(y_test, tuning_three.predict(X_test))*100} %')

"""### Tuning 4 (Combination of Tuning 2 and Tuning 3)"""

preds_x = tuning_two.predict(X_test)
preds_y = tuning_three.predict(X_test)

best_weight_x_arr = np.arange(0.0, 1, 0.01)
best_weight_y_arr = np.arange(1, 0.0, -0.01)
best_mape_arr = []

best_weight_x = 0
best_mape_four = 100

for j in np.arange(0.0, 1, 0.01):
  weight_x = j
  weight_y = 1-weight_x

  final_preds = []

  for i in range(len(X_test)):
    pred = (weight_x*preds_x[i]) + (weight_y*preds_y[i])
    final_preds.append(pred)

  mape = mean_absolute_percentage_error(y_test, final_preds)*100
  best_mape_arr.append(mape)
  if (mape < best_mape_four):
    best_mape_four = mape
    best_weight_x = weight_x


print(f'Best Weight One: {best_weight_x}, Best Weight Two: {1-best_weight_x}')
print(f'Best MAPE: {best_mape_four}')
#print(f'Combined MAPE: {mean_absolute_percentage_error(y_test, final_preds)*100} %')


print(f'Tuning 2 MAPE: {mean_absolute_percentage_error(y_test, tuning_two.predict(X_test))*100} %')
print(f'Tuning 3 MAPE: {mean_absolute_percentage_error(y_test, tuning_three.predict(X_test))*100} %')
#print(f'Combined MAPE: {mean_absolute_percentage_error(y_test, final_preds)*100} %')

plt.figure(figsize=(15,10))

plt.plot(best_weight_x_arr*100, best_mape_arr, label = 'Tuning #2')
plt.plot(best_weight_y_arr*100, best_mape_arr, label = 'Tuning #3')
plt.xlabel('Set Weight Weight %')
plt.ylabel('Test MAPE')
plt.title('Set Weight vs Test MAPE ')
plt.legend()

"""### Tuning 5 (Lowering Iterations)"""

cv = RepeatedKFold(n_splits = 3, random_state = 42, n_repeats=2)
model = CatBoostRegressor(logging_level='Silent', loss_function='MAPE')

grid = {'iterations': [100, 150, 200],
        'learning_rate': [0.03, 0.1],
        'depth': [2, 4, 6, 8],
        'l2_leaf_reg': [0.2, 0.5, 1, 3]}

eval_pool = Pool(X_test, y_test)

fit_params = {'eval_set' : eval_pool,
              'early_stopping_rounds' : 10
             }


search = GridSearchCV(model, grid, scoring='neg_mean_absolute_percentage_error', cv=cv, verbose=3)

tuning_five = search.fit(X_train, y_train, **fit_params)

tuning_five.best_params_

tuning_five = CatBoostRegressor(depth=4, iterations=150, l2_leaf_reg=3, learning_rate=0.1)
tuning_five.fit(X_train, y_train)

print(f'Test MAPE: {mean_absolute_percentage_error(y_test, tuning_five.predict(X_test))*100} %')

"""### Tuning 6 (3 and 5)"""

preds_x = tuning_three.predict(X_test)
preds_y = tuning_five.predict(X_test)

best_weight_x_arr = np.arange(0.0, 1, 0.01)
best_weight_y_arr = np.arange(1, 0.0, -0.01)
best_mape_arr = []

best_weight_x = 0
best_mape_six = 100

for j in np.arange(0.0, 1, 0.01):
  weight_x = j
  weight_y = 1-weight_x

  final_preds = []

  for i in range(len(X_test)):
    pred = (weight_x*preds_x[i]) + (weight_y*preds_y[i])
    final_preds.append(pred)

  mape = mean_absolute_percentage_error(y_test, final_preds)*100
  best_mape_arr.append(mape)
  if (mape < best_mape_six):
    best_mape_six = mape
    best_weight_x = weight_x


print(f'Best Weight One: {best_weight_x}, Best Weight Two: {1-best_weight_x}')
print(f'Best MAPE: {best_mape_six}')
#print(f'Combined MAPE: {mean_absolute_percentage_error(y_test, final_preds)*100} %')


print(f'Tuning 3 MAPE: {mean_absolute_percentage_error(y_test, tuning_three.predict(X_test))*100} %')
print(f'Tuning 5 MAPE: {mean_absolute_percentage_error(y_test, tuning_five.predict(X_test))*100} %')
#print(f'Combined MAPE: {mean_absolute_percentage_error(y_test, final_preds)*100} %')

plt.figure(figsize=(15,10))

plt.plot(best_weight_x_arr*100, best_mape_arr, label = 'Tuning #3')
plt.plot(best_weight_y_arr*100, best_mape_arr, label = 'Tuning #5')
plt.xlabel('Set Weight Weight %')
plt.ylabel('Test MAPE')
plt.title('Set Weight vs Test MAPE ')
plt.legend()

"""### Results"""

models_list = {'Tuning #': ['Untuned', 'Tuning #1', 'Tuning #2', 'Tuning #3', 'Tuning #4', 'Tuning #5', 'Tuning #6']}

results = pd.DataFrame(data = models_list)

linreg = np.zeros(7)
results['Best MAPE'] = linreg.copy()

results.iat[0, 1] = mean_absolute_percentage_error(y_test, untuned.predict(X_test))*100
results.iat[1, 1] = mean_absolute_percentage_error(y_test, tuning_one.predict(X_test))*100
results.iat[2, 1] = mean_absolute_percentage_error(y_test, tuning_two.predict(X_test))*100
results.iat[3, 1] = mean_absolute_percentage_error(y_test, tuning_three.predict(X_test))*100
results.iat[4, 1] = best_mape_four
results.iat[5, 1] = mean_absolute_percentage_error(y_test, tuning_five.predict(X_test))*100
results.iat[6, 1] = best_mape_six


display(results)

"""## Evaluation

### Final Predictions
"""

X_train, X_test, y_train, y_test = train_test_split(onethirty_features.drop(columns=['Log Value', 'Player']), onethirty_features['Log Value'], random_state=42, test_size=0.1)

tuning_three = CatBoostRegressor(depth=5, iterations=1250,  learning_rate = 0.032)
tuning_three.fit(X_train, y_train)

tuning_five = CatBoostRegressor(depth=4, iterations=150, l2_leaf_reg=3, learning_rate=0.1)
tuning_five.fit(X_train, y_train)

preds_x_train = tuning_three.predict(X_train)
preds_y_train = tuning_five.predict(X_train)

final_preds_train = preds_x_train * 0.79 + preds_y_train * 0.21


preds_x_test = tuning_three.predict(X_test)
preds_y_test = tuning_five.predict(X_test)

final_preds_test = preds_x_test * 0.79 + preds_y_test * 0.21

mean_absolute_percentage_error(y_test, final_preds_test)*100

preds_test = np.e ** final_preds_test
actual = np.e ** y_test

name_train, name_test, y_train, y_test = train_test_split(onethirty_features['Player'], onethirty_features['Log Value'], random_state=42, test_size=0.1)

results = pd.DataFrame(name_test)
results['Predicted Value'] = preds_test
results['Actual Value'] = actual
results['Error'] = abs(preds_test-actual)
results["Error %"] = abs(preds_test-actual)/actual * 100

pd.options.display.float_format = '{:,}'.format
results.round(2)

results = results.sort_values(by=['Error %'])
results = results.round(2)
results

results.head(5).append(results.tail(5))

"""### Feature Importance Evaluation"""

importance_three = tuning_three.get_feature_importance()
importance_five = tuning_five.get_feature_importance()

cols = onethirty_features.drop(columns=['Player', 'Log Value']).columns

feature_importance = pd.DataFrame(cols, columns=['Feature'])
feature_importance['Importance %'] = importance_three * 0.79 + importance_five * 0.21
feature_importance

feature_importance = feature_importance.sort_values(by=['Importance %'], ascending=False)
feature_importance = feature_importance.round(2)
feature_importance

feature_importance.head(5).append(feature_importance.tail(5))