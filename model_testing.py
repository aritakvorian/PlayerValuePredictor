# -*- coding: utf-8 -*-
"""Model Testing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YcsBIDZVFc8dm8zmtRGT44Wv6z08wSuw

# Player Transfermrket Value Estimator
"""

!pip install -U scikit-learn

import pandas as pd
import math
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error
import xgboost as xgb
pd.set_option("display.max_columns", None, "display.max_rows", None)

from google.colab import drive
drive.mount('/content/gdrive')

cd gdrive/MyDrive/UT/Plan\ II\ \(General\)/Thesis/Model

raw = pd.read_csv('final_dataset.csv')

all_features = pd.read_csv('all_features.csv')
oneten_features = pd.read_csv('oneten_features.csv')
eighty_features = pd.read_csv('eighty_features.csv')
fifty_features = pd.read_csv('fifty_features.csv')
twenty_features = pd.read_csv('twenty_features.csv')

"""## Model Testing

### Setup

Will put each of the five data frames through the following models (with minimal tuning - real tuning will come after choosing the best type of model):

*   Multiple (Simple) Linear Regression
*   Lasso Regression
*   Ridge Regression
*   XGBoost Regressor
*   ElasticNet Regression
*   CatBoost Regressor
*   Random Forest Regressor
*   K-Nearest Neighbors

The regression metric used will be the Mean Absolute Percentage Error (MAPE), but, because I have log-transformed the data, it will reflect the log percentage.

Each dataset will be split into train and test sets (test_size being 10% of data). Will train on the 90%, and evaluate the MAPE on the remaining 10%).
"""

models_list = {'Feature #': ['All_Features', '110_Features', '80_Features', '50_Features', '20_Features']}

results = pd.DataFrame(data = models_list)

linreg = np.zeros(5)
results['LinReg'] = linreg.copy()
results['Lasso'] = linreg.copy()
results['Ridge'] = linreg.copy()
results['XGBoost'] = linreg.copy()
results['ElasticNet'] = linreg.copy()
results['CatBoost'] = linreg.copy()
results['RandomForest'] = linreg.copy()
results['KNN'] = linreg.copy()

results.head()

# Creating Train-Test Splits

X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(all_features.drop(columns=['Log Value']), all_features['Log Value'], random_state = 42, test_size = 0.10)
X_train_oneten, X_test_oneten, y_train_oneten, y_test_oneten = train_test_split(oneten_features.drop(columns=['Log Value']), oneten_features['Log Value'], random_state = 42, test_size = 0.10)
X_train_eighty, X_test_eighty, y_train_eighty, y_test_eighty = train_test_split(eighty_features.drop(columns=['Log Value']), eighty_features['Log Value'], random_state = 42, test_size = 0.10)
X_train_fifty, X_test_fifty, y_train_fifty, y_test_fifty = train_test_split(fifty_features.drop(columns=['Log Value']), fifty_features['Log Value'], random_state = 42, test_size = 0.10)
X_train_twenty, X_test_twenty, y_train_twenty, y_test_twenty = train_test_split(twenty_features.drop(columns=['Log Value']), twenty_features['Log Value'], random_state = 42, test_size = 0.10)

"""### Multiple (Simple) Linear Regression"""

from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression

model = LinearRegression()

## All Features - MLR

model.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, model.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[0,1] = mape_test*100

## 110 Features - MLR

model.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, model.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test}')
results.iat[1,1] = mape_test*100

## 80 Features - MLR

model.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, model.predict(X_test_eighty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[2,1] = mape_test*100

## 50 Features - MLR

model.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, model.predict(X_test_fifty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[3,1] = mape_test*100

## 20 Features - MLR

model.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, model.predict(X_test_twenty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[4,1] = mape_test*100

results

"""### Lasso Regression"""

from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Lasso

grid = {'alpha': [0, 0.01, 0.1, 1]}
cv = RepeatedKFold(n_splits = 3, n_repeats = 3, random_state = 42)
model = Lasso()

search = GridSearchCV(model, grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs =-1)

## All Features - Lasso

res_all = search.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, search.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test} with Hyperparameters: {res_all.best_params_}')
results.iat[0,2] = mape_test*100

## 110 Features - Lasso

res_oneten = search.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, search.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test} with Hyperparameters: {res_oneten.best_params_}')
results.iat[1,2] = mape_test*100

## 80 Features - Lasso

res_eighty = search.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, search.predict(X_test_eighty))

print(f'Test Data MAPE for 80 Features: {mape_test} with Hyperparameters: {res_eighty.best_params_}')
results.iat[2,2] = mape_test*100

## 50 Features - Lasso

res_fifty = search.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, search.predict(X_test_fifty))

print(f'Test Data MAPE for 50 Features: {mape_test} with Hyperparameters: {res_fifty.best_params_}')
results.iat[3,2] = mape_test*100

## 20 Features - Lasso

res_twenty = search.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, search.predict(X_test_twenty))

print(f'Test Data MAPE for 20 Features: {mape_test} with Hyperparameters: {res_twenty.best_params_}')
results.iat[4,2] = mape_test*100

"""### Ride Regression"""

from sklearn.linear_model import Ridge

grid = {'alpha': np.arange(0, 1, 0.05)}
cv = RepeatedKFold(n_splits = 3, n_repeats = 3, random_state = 42)
model = Ridge()

search = GridSearchCV(model, grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs =-1)

## All Features - Ridge

res_all = search.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, search.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test} with Hyperparameters: {res_all.best_params_}')
results.iat[0,3] = mape_test*100

## 110 Features - Ridge

res_oneten = search.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, search.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test} with Hyperparameters: {res_oneten.best_params_}')
results.iat[1,3] = mape_test*100

## 80 Features - Ridge

res_eighty = search.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, search.predict(X_test_eighty))

print(f'Test Data MAPE for 80 Features: {mape_test} with Hyperparameters: {res_eighty.best_params_}')
results.iat[2,3] = mape_test*100

## 50 Features - Ridge

res_fifty = search.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, search.predict(X_test_fifty))

print(f'Test Data MAPE for 50 Features: {mape_test} with Hyperparameters: {res_fifty.best_params_}')
results.iat[3,3] = mape_test*100

## 20 Features - Ridge

res_twenty = search.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, search.predict(X_test_twenty))

print(f'Test Data MAPE for 20 Features: {mape_test} with Hyperparameters: {res_twenty.best_params_}')
results.iat[4,3] = mape_test*100

"""### XGBoost"""

import xgboost
from xgboost import XGBRegressor

model = XGBRegressor(objective='reg:squarederror')

## All Features - XGBoost

model.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, model.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[0,4] = mape_test*100

## 110 Features - XGBoost

model.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, model.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test}')
results.iat[1,4] = mape_test*100

## 80 Features - XGBoost

model.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, model.predict(X_test_eighty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[2,4] = mape_test*100

## 50 Features - XGBoost

model.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, model.predict(X_test_fifty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[3,4] = mape_test*100

## 20 Features - XGBoost

model.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, model.predict(X_test_twenty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[4,4] = mape_test*100

"""### ElasticNet"""

from sklearn.linear_model import ElasticNet

grid = {'alpha': np.arange(0, 1, 0.2), 
        'l1_ratio': np.arange(0, 1, 0.5)}
cv = RepeatedKFold(n_splits = 3, n_repeats = 3, random_state = 42)
model = ElasticNet()

search = GridSearchCV(model, grid, scoring='neg_mean_absolute_percentage_error', cv=cv, n_jobs =-1)

## All Features - ElasticNet

res_all = search.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, search.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test} with Hyperparameters: {res_all.best_params_}')
results.iat[0,5] = mape_test*100

## 110 Features - ElasticNet

res_oneten = search.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, search.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test} with Hyperparameters: {res_oneten.best_params_}')
results.iat[1,5] = mape_test*100

## 80 Features - ElasticNet

res_eighty = search.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, search.predict(X_test_eighty))

print(f'Test Data MAPE for 80 Features: {mape_test} with Hyperparameters: {res_eighty.best_params_}')
results.iat[2,5] = mape_test*100

## 50 Features - ElasticNet

res_fifty = search.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, search.predict(X_test_fifty))

print(f'Test Data MAPE for 50 Features: {mape_test} with Hyperparameters: {res_fifty.best_params_}')
results.iat[3,5] = mape_test*100

## 20 Features - ElasticNet

res_twenty = search.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, search.predict(X_test_twenty))

print(f'Test Data MAPE for 20 Features: {mape_test} with Hyperparameters: {res_twenty.best_params_}')
results.iat[4,5] = mape_test*100

results

"""### CatBoost """

pip install catboost

import catboost
from catboost import CatBoostRegressor

model = CatBoostRegressor(loss_function = 'MAPE')

## All Features - CatBoost

model.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, model.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[0,6] = mape_test*100

## 110 Features - CatBoost

model.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, model.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test}')
results.iat[1,6] = mape_test*100

## 80 Features - CatBoost

model.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, model.predict(X_test_eighty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[2,6] = mape_test*100

## 50 Features - CatBoost

model.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, model.predict(X_test_fifty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[3,6] = mape_test*100

## 20 Features - CatBoost

model.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, model.predict(X_test_twenty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[4,6] = mape_test*100

results

"""### Random Forest"""

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(random_state = 42, criterion='absolute_error')

## All Features - Random Forest

model.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, model.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[0,7] = mape_test*100

## 110 Features - Random Forest

model.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, model.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test}')
results.iat[1,7] = mape_test*100

## 80 Features - Random Forest

model.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, model.predict(X_test_eighty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[2,7] = mape_test*100

## 50 Features - Random Forest

model.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, model.predict(X_test_fifty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[3,7] = mape_test*100

## 20 Features - Random Forest

model.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, model.predict(X_test_twenty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[4,7] = mape_test*100

results

"""### K-Nearest Neighbors



"""

from sklearn.neighbors import KNeighborsRegressor

model = KNeighborsRegressor()

## All Features - KNN

model.fit(X_train_all, y_train_all)

mape_test = mean_absolute_percentage_error(y_test_all, model.predict(X_test_all))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[0,8] = mape_test*100

## 110 Features - KNN

model.fit(X_train_oneten, y_train_oneten)

mape_test = mean_absolute_percentage_error(y_test_oneten, model.predict(X_test_oneten))

print(f'Test Data MAPE for 110 Features: {mape_test}')
results.iat[1,8] = mape_test*100

## 80 Features - KNN

model.fit(X_train_eighty, y_train_eighty)

mape_test = mean_absolute_percentage_error(y_test_eighty, model.predict(X_test_eighty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[2,8] = mape_test*100

## 50 Features - KNN

model.fit(X_train_fifty, y_train_fifty)

mape_test = mean_absolute_percentage_error(y_test_fifty, model.predict(X_test_fifty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[3,8] = mape_test*100

## 20 Features - KNN

model.fit(X_train_twenty, y_train_twenty)

mape_test = mean_absolute_percentage_error(y_test_twenty, model.predict(X_test_twenty))

print(f'Test Data MAPE for All Features: {mape_test}')
results.iat[4,8] = mape_test*100

results

"""### Conclusion of Model Selection"""

display(results)

plt.figure(figsize=(15,10))
plt.plot(results['Features #'], results['LinReg'], label='Linear Regression')
plt.plot(results['Features #'], results['Lasso'], label='Lasso Regression')
plt.plot(results['Features #'], results['Ridge'], label='Ridge Regression')
plt.plot(results['Features #'], results['XGBoost'], label='XGBoost')
plt.plot(results['Features #'], results['ElasticNet'], label='ElasticNet Regression')
plt.plot(results['Features #'], results['CatBoost'], label='CatBoost')
plt.plot(results['Features #'], results['RandomForest'], label='Random Forest')
plt.plot(results['Features #'], results['KNN'], label='K-Nearest Neighbors')
plt.xlabel('Number of Features')
plt.ylabel('Mean Absolute Percentage Error (MAPE) for Log Value')
plt.title('Model Framework MAPE Performance by # of Features')
plt.legend()

"""CatBoost seems to work the best, and on the 110 feature set. This indicates that there is some best value for feature #s. Going to try some more #s of features between All and 110 for CatBoost to determine the best data set.

## Further Feature Selection

### Features

Going to compare results for CatBoost for features counts:

 All, 130, 125, 120, 115, and 110.

*   All
*   130
*   125
*   120
*   115
*   110

All and 110 are already calculated
"""

# 130 Feats

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

select_onethirty = SelectKBest(score_func=f_regression, k = 130)
onethirty = select_onethirty.fit_transform(all_features.drop(columns=['Log Value']), all_features['Log Value'])

filter = select_onethirty.get_support()
feats = np.array(all_features.drop(columns=['Log Value']).columns)

#print(f'Selected best 130 features: {feats[filter]}')

onethirty_features = all_features.copy()
onethirty_features.drop(onethirty_features.columns.difference(feats[filter]), 1, inplace=True)
onethirty_features['Log Value'] = all_features['Log Value'].copy()

# 125 Feats

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

select_onetwofive = SelectKBest(score_func=f_regression, k = 125)
onetwofive = select_onetwofive.fit_transform(all_features.drop(columns=['Log Value']), all_features['Log Value'])

filter = select_onetwofive.get_support()
feats = np.array(all_features.drop(columns=['Log Value']).columns)

#print(f'Selected best 125 features: {feats[filter]}')

onetwofive_features = all_features.copy()
onetwofive_features.drop(onetwofive_features.columns.difference(feats[filter]), 1, inplace=True)
onetwofive_features['Log Value'] = all_features['Log Value'].copy()

# 120 Feats

select_onetwenty = SelectKBest(score_func=f_regression, k = 120)
onetwenty = select_onetwenty.fit_transform(all_features.drop(columns=['Log Value']), all_features['Log Value'])

filter = select_onetwenty.get_support()
feats = np.array(all_features.drop(columns=['Log Value']).columns)

#print(f'Selected best 120 features: {feats[filter]}')

onetwenty_features = all_features.copy()
onetwenty_features.drop(onetwenty_features.columns.difference(feats[filter]), 1, inplace=True)
onetwenty_features['Log Value'] = all_features['Log Value'].copy()

# 115 Feats

select_onefifteen = SelectKBest(score_func=f_regression, k = 115)
onefifteen = select_onefifteen.fit_transform(all_features.drop(columns=['Log Value']), all_features['Log Value'])

filter = select_onefifteen.get_support()
feats = np.array(all_features.drop(columns=['Log Value']).columns)

#print(f'Selected best 115 features: {feats[filter]}')

onefifteen_features = all_features.copy()
onefifteen_features.drop(onefifteen_features.columns.difference(feats[filter]), 1, inplace=True)
onefifteen_features['Log Value'] = all_features['Log Value'].copy()

"""### Model Testing pt 2

Going to repeat the process as before, but only for CatBoost
"""

X_train_onethirty, X_test_onethirty, y_train_onethirty, y_test_onethirty = train_test_split(onethirty_features.drop(columns=['Log Value']), onethirty_features['Log Value'], random_state = 42, test_size = 0.10)
X_train_onetwofive, X_test_onetwofive, y_train_onetwofive, y_test_onetwofive = train_test_split(onetwofive_features.drop(columns=['Log Value']), onetwofive_features['Log Value'], random_state = 42, test_size = 0.10)
X_train_onetwenty, X_test_onetwenty, y_train_onetwenty, y_test_onetwenty = train_test_split(onetwenty_features.drop(columns=['Log Value']), onetwenty_features['Log Value'], random_state = 42, test_size = 0.10)
X_train_onefifteen, X_test_onefifteen, y_train_onefifteen, y_test_onefifteen = train_test_split(onefifteen_features.drop(columns=['Log Value']), onefifteen_features['Log Value'], random_state = 42, test_size = 0.10)

models_list = {'Feature #': ['All_Features', '130_Features', '125_Features','120_Features', '115_Features', '110_Features']}

results_two = pd.DataFrame(data = models_list)

linreg = np.zeros(6)
results_two['CatBoost'] = linreg.copy()

results_two.iat[0, 1] = results.iloc[0, 6]
results_two.iat[5, 1] = results.iloc[1, 6]

display(results_two)

import catboost
from catboost import CatBoostRegressor

model = CatBoostRegressor(loss_function = 'MAPE')

## 130 Features - CatBoost

model.fit(X_train_onethirty, y_train_onethirty)

mape_test = mean_absolute_percentage_error(y_test_onethirty, model.predict(X_test_onethirty))

print(f'Test Data MAPE for 130 Features: {mape_test}')
results_two.iat[1,1] = mape_test*100

## 125 Features - CatBoost

model.fit(X_train_onetwofive, y_train_onetwofive)

mape_test = mean_absolute_percentage_error(y_test_onetwofive, model.predict(X_test_onetwofive))

print(f'Test Data MAPE for 125 Features: {mape_test}')
results_two.iat[2,1] = mape_test*100

## 120 Features - CatBoost

model.fit(X_train_onetwenty, y_train_onetwenty)

mape_test = mean_absolute_percentage_error(y_test_onetwenty, model.predict(X_test_onetwenty))

print(f'Test Data MAPE for 120 Features: {mape_test}')
results_two.iat[3,1] = mape_test*100

## 115 Features - CatBoost

model.fit(X_train_onefifteen, y_train_onefifteen)

mape_test = mean_absolute_percentage_error(y_test_onefifteen, model.predict(X_test_onefifteen))

print(f'Test Data MAPE for 115 Features: {mape_test}')
results_two.iat[4,1] = mape_test*100

results_two

"""### Conclusion

The results show the best feature # with a non-tuned CatBoost is 130 (barely). As a result, I am going to continue with a CatBoostRegression with 130 features.
"""

onethirty_features.to_csv('onethirty_features.csv', encoding='utf-8', index=False)